# Architecture technique du projet Casimir â€” Mairie Pierrefonds

Ce document dÃ©crit lâ€™architecture du projet, les composants, le pipeline dâ€™indexation et le dÃ©ploiement.

---

## 1. Vue dâ€™ensemble

Le projet comporte **deux frontends** et un **pipeline dâ€™ingestion** commun :

| Composant | RÃ´le | Stack |
|-----------|------|--------|
| **Application principale** | Interface utilisateur (recherche, agent, stats, sources) | Streamlit, Plotly |
| **Application Django** | Recherche simple (alternative lÃ©gÃ¨re) | Django, `web/search` |
| **Pipeline dâ€™indexation** | Alimentation de la base vectorielle et des stats | Python, `ingest.py`, `fetch_sites.py`, `stats_extract.py` |

La **base vectorielle** est construite par `ingest.py` et consommÃ©e par lâ€™app Streamlit (et optionnellement par Django via une base au format `.npz`/`.json` distincte dans `build_vector_store.py`).

---

## 2. Structure des rÃ©pertoires

```
Mairie/
â”œâ”€â”€ app.py                    # Application Streamlit (Casimir)
â”œâ”€â”€ ingest.py                 # Indexation .md + PDF â†’ vector_db/
â”œâ”€â”€ build_vector_store.py     # Base vectorielle alternative (Django, format .npz/.json)
â”œâ”€â”€ fetch_sites.py            # RÃ©cupÃ©ration URLs â†’ knowledge_sites/*.md
â”œâ”€â”€ copy_md_to_static.py      # Copie .md â†’ static/ pour listing Sources
â”œâ”€â”€ stats_extract.py          # Extraction stats PV â†’ vector_db/stats.json
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ siteweb.txt / site_url.txt  # Liste dâ€™URLs Ã  scraper
â”œâ”€â”€ deploy_date.txt           # Date de dÃ©ploiement (Ã©crit par deploy.bat)
â”œâ”€â”€ static/                   # Fichiers statiques (PDF, .md copiÃ©s, Guide-utilisateurs.md)
â”‚   â””â”€â”€ journal/              # PDFs Lâ€™ECHO (copiÃ©s par ingest)
â”œâ”€â”€ knowledge_sites/          # Fichiers .md issus de fetch_sites.py
â”œâ”€â”€ journal/                  # PDFs Lâ€™ECHO (source) + download_calameo.py
â”œâ”€â”€ vector_db/                # Base vectorielle (sortie de ingest.py)
â”‚   â”œâ”€â”€ embeddings.npy        # Matrice (N, dim) float32 normalisÃ©e
â”‚   â”œâ”€â”€ documents.pkl        # Liste de N textes (chunks)
â”‚   â”œâ”€â”€ metadata.pkl         # Liste de N mÃ©tadonnÃ©es (filename, date, year, chunk, â€¦)
â”‚   â””â”€â”€ stats.json           # Stats sÃ©ances/dÃ©libÃ©rations (sortie stats_extract.py)
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ Guide-utilisateurs.md
â”‚   â”œâ”€â”€ Architecture-technique.md
â”‚   â””â”€â”€ Recherche-et-agent-RAG.md
â”œâ”€â”€ web/                      # Application Django (recherche)
â”‚   â”œâ”€â”€ config/               # Settings, urls
â”‚   â”œâ”€â”€ search/               # App recherche (views, vector_search, templates)
â”‚   â””â”€â”€ manage.py
â”œâ”€â”€ ALL.bat                   # Pipeline complet : URL â†’ Guide â†’ Update_Casimir â†’ Deploy
â”œâ”€â”€ URL.bat                   # fetch_sites.py (site_url.txt â†’ knowledge_sites/)
â”œâ”€â”€ Update_Casimir.bat        # ingest + copy_md + stats_extract
â””â”€â”€ deploy.bat                # deploy_date, copy_md, git commit/push â†’ Streamlit Cloud
```

---

## 3. Pipeline dâ€™indexation

### 3.1 Script ALL.bat (pipeline complet)

Ordre dâ€™exÃ©cution :

1. **URL** : `URL.bat` â†’ lecture de `site_url.txt` (ou `siteweb.txt`), appel Ã  `fetch_sites.py` â†’ gÃ©nÃ©ration des `.md` dans `knowledge_sites/`.
2. **Guide utilisateur** : copie de `docs/Guide-utilisateurs.md` vers `static/Guide-utilisateurs.md` (pour la popup du site).
3. **Update_Casimir** : `Update_Casimir.bat` â†’ `ingest.py --md-only`, puis optionnellement `ingest.py` (PDFs), `copy_md_to_static.py`, `stats_extract.py` si `vector_db/stats.json` absent.
4. **Deploy** : `deploy.bat` â†’ mise Ã  jour `deploy_date.txt`, `copy_md_to_static.py`, git add/commit/push â†’ redÃ©ploiement Streamlit Cloud.

### 3.2 fetch_sites.py â€” RÃ©cupÃ©ration des pages web

- **EntrÃ©e** : `site_url.txt` ou `siteweb.txt` (une URL par ligne).
- **Sortie** : un fichier `.md` par URL dans `knowledge_sites/`, avec en-tÃªte `Source : <url>` et contenu texte extrait du HTML.

StratÃ©gies de rÃ©cupÃ©ration (par ordre de prioritÃ© selon le domaine) :

- **Playwright** (headless Chromium) pour les domaines listÃ©s dans `JS_RENDER_DOMAINS` (ex. `notion.site`, `tripadvisor.fr`) : exÃ©cution du JavaScript, contournement anti-bot.
- **curl_cffi** (impersonation TLS Chrome) pour `TLS_IMPERSONATE_DOMAINS` (ex. `courrier-picard.fr`) : Ã©vite les 403.
- **requests** pour les autres URLs.
- **Fallback** : si Ã©chec (403, contenu vide), appel Ã  ScraperAPI ou ZenRows si les variables dâ€™environnement `SCRAPER_API_KEY` ou `ZENROWS_API_KEY` sont dÃ©finies.

Les domaines dans `SKIP_DOMAINS` (ex. `facebook.com`) ne sont pas traitÃ©s.

### 3.3 ingest.py â€” Indexation vers la base vectorielle

- **EntrÃ©e** :
  - Fichiers `.md` dans `knowledge_sites/` (toujours indexÃ©s en premier).
  - PDFs dans `static/` et `static/journal/` (si pas `--md-only`).
- **Sortie** : `vector_db/embeddings.npy`, `vector_db/documents.pkl`, `vector_db/metadata.pkl`.

Ã‰tapes :

1. **Copie des PDFs journal** : `journal/*.pdf` â†’ `static/journal/` pour servir les PDFs cÃ´tÃ© Streamlit.
2. **Chargement du modÃ¨le** : `sentence-transformers` avec `paraphrase-multilingual-MiniLM-L12-v2` (CPU ou GPU si `USE_GPU` et CUDA).
3. **Traitement des .md** : lecture, extraction du contenu aprÃ¨s `---`, dÃ©coupage en chunks (voir document Â« Recherche et agent RAG Â»), mÃ©tadonnÃ©es `filename` prÃ©fixÃ© `[Web]`, `source_url` si prÃ©sent.
4. **Traitement des PDFs** : extraction de texte avec `pdfplumber` ; pour les PDFs image (ex. Lâ€™ECHO), OCR via Tesseract puis EasyOCR en secours (activÃ© par `INGEST_OCR_JOURNAL=1`). DÃ©coupage en chunks, extraction de la date depuis le nom de fichier (`extract_date`).
5. **Embeddings** : encodage par batch (64 textes), normalisation L2 pour similaritÃ© cosinus.
6. **Sauvegarde** : `np.save(embeddings.npy)`, `pickle.dump(documents.pkl)`, `pickle.dump(metadata.pkl)`.

ParamÃ¨tres clÃ©s : `CHUNK_SIZE = 1000` (caractÃ¨res), chunks de moins de 80 caractÃ¨res exclus.

### 3.4 copy_md_to_static.py

Copie tous les `.md` de la racine (hors README) et de `knowledge_sites/` vers `static/` et `static/knowledge_sites/` pour que lâ€™app Streamlit puisse les lister et les servir dans la section Â« Sources et Documents Â».

### 3.5 stats_extract.py

Parcourt les PDFs dans `static/` (procÃ¨s-verbaux), extrait avec `pdfplumber` et des regex :

- Date de sÃ©ance, horaires (dÃ©but/fin), durÃ©e.
- Liste des prÃ©sents, absents, pouvoirs.
- DÃ©libÃ©rations (titre, thÃ¨me via `THEME_PATTERNS`, type de vote : unanimitÃ© / vote avec dÃ©compte, pour/contre/abstentions, noms).

Produit `vector_db/stats.json` utilisÃ© par la section Â« Statistiques des sÃ©ances Â» de lâ€™app Streamlit.

---

## 4. Application Streamlit (app.py)

- **Page config** : `st.set_page_config(layout="wide", page_icon="ğŸ›ï¸")`.
- **Ã‰tat** : `st.session_state["current_section"]` = `home` | `agent` | `search` | `stats` | `docs`.
- **Ressources cachÃ©es** : `load_model()` et `load_db()` en `@st.cache_resource` (modÃ¨le SentenceTransformer, chargement de `vector_db/`).
- **Bandeau** : Accueil, Ã€ propos, Guide Utilisateur, email, date de dÃ©ploiement, IP (via ipify), compteur de recherches et quota restant (rate limit).
- **Rate limiting** : 5 recherches/heure par IP (sauf whitelist `RATE_LIMIT_WHITELIST`), stockage en mÃ©moire des timestamps par IP.
- **Mode admin** : `?admin=<token>` avec `ADMIN_TOKEN` dans `st.secrets` ; affichage dâ€™infos supplÃ©mentaires (ex. nombre de passages indexÃ©s).

Fichiers statiques : les PDFs sont servis sous `app/static/` (Streamlit Cloud) ; les URLs sont gÃ©nÃ©rÃ©es via `_safe_pdf_url(rel_path)` pour Ã©viter path traversal et schÃ©mas dangereux.

---

## 5. Application Django (web/)

- **RÃ´le** : interface de recherche alternative (formulaire + rÃ©sultats), sans agent ni stats.
- **Base** : par dÃ©faut `BASE_VECTORIELLE` pointe vers `base_vectorielle/` (gÃ©nÃ©rÃ©e par `build_vector_store.py`), avec format `.npz` + `metadata.json` (structure diffÃ©rente de `ingest.py`). Pour utiliser la mÃªme base que Streamlit, il faudrait adapter `vector_search.py` pour lire `vector_db/embeddings.npy` + `documents.pkl` + `metadata.pkl`.
- **Recherche** : `vector_search.search()` avec seuils `MIN_SIMILARITY`, `MIN_SIMILARITY_IF_KEYWORD_MATCH`, filtre par mots-clÃ©s pour requÃªtes courtes.

---

## 6. DÃ©ploiement (deploy.bat)

1. Mise Ã  jour de Streamlit (`pip install -U streamlit`).
2. CrÃ©ation du dossier `data/` si absent.
3. Ã‰criture de la date dans `deploy_date.txt`.
4. ExÃ©cution de `copy_md_to_static.py`.
5. `git add -A`, commit (message demandÃ© ou automatique), `git pull --rebase`, `git push origin main`.

Streamlit Cloud dÃ©ploie automatiquement Ã  partir du dÃ©pÃ´t GitHub (branch `main`). Les secrets (ex. `GROQ_API_KEY`, `ADMIN_TOKEN`) sont Ã  configurer dans le dashboard Streamlit Cloud.

---

## 7. DÃ©pendances principales (requirements.txt)

- **Recherche / embeddings** : `sentence-transformers`, `numpy`.
- **Interface** : `streamlit`, `plotly`, `streamlit-javascript`.
- **Agent** : `groq`.
- **PDF** : `pdfplumber`, `PyMuPDF`, `Pillow`.
- **OCR** : `easyocr`, `pytesseract`.
- **Scraping** : `requests`, `beautifulsoup4`, `curl_cffi`, `playwright`.

Pour lâ€™OCR des journaux, Tesseract peut Ãªtre installÃ© cÃ´tÃ© systÃ¨me (Windows : binaire Tesseract-OCR) ; sinon EasyOCR suffit (`pip install easyocr`). Pour Playwright : `playwright install chromium`.

---

## 8. Variables dâ€™environnement et secrets

| Variable / secret | Usage |
|-------------------|--------|
| `INGEST_OCR_JOURNAL` | `1` pour activer lâ€™OCR des PDFs Lâ€™ECHO dans `ingest.py`. |
| `USE_GPU` | PrÃ©sent et CUDA disponible â†’ modÃ¨le SentenceTransformer sur GPU. |
| `SCRAPER_API_KEY` / `ZENROWS_API_KEY` | Fallback scraping dans `fetch_sites.py` en cas dâ€™Ã©chec direct. |
| `GROQ_API_KEY` (Streamlit secrets) | Appel API Groq pour lâ€™agent (llama-3.3-70b-versatile). |
| `ADMIN_TOKEN` (Streamlit secrets) | AccÃ¨s mode admin via `?admin=<token>`. |

---

*Documentation technique â€” projet Casimir, Mairie Pierrefonds.*
